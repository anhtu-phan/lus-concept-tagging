{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Concept tagging with Weighted Finite State Machines"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from conll import evaluate\n",
    "import pandas as pd\n",
    "import os\n",
    "from utils import *\n",
    "from pre_process_data import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preprocess data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "norm_data_input(\"dataset/NL2SparQL4NLU.train_no_stop_word.utterances.txt\",\n",
    "                \"dataset/NL2SparQL4NLU.train_norm_all_words_no_stop_word.utterances.txt\")\n",
    "norm_data_input(\"dataset/NL2SparQL4NLU.test_no_stop_word.utterances.txt\",\n",
    "                \"dataset/NL2SparQL4NLU.test_norm_all_words_no_stop_word.utterances.txt\")\n",
    "norm_data_input(\"dataset/NL2SparQL4NLU.train_no_stop_word.conll.txt\",\n",
    "                \"dataset/NL2SparQL4NLU.train_norm_all_words_no_stop_word.conll.txt\",\n",
    "                file_type='conll')\n",
    "norm_data_input(\"dataset/NL2SparQL4NLU.test_no_stop_word.conll.txt\",\n",
    "                \"dataset/NL2SparQL4NLU.test_norm_all_words_no_stop_word.conll.txt\",\n",
    "                file_type='conll')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preparing Input Symbol Tables (`isyms.txt`)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "dpath='NL2SparQL4NLU/dataset/NL2SparQL4NLU'\n",
    "dtype=''\n",
    "\n",
    "cp $dpath.train$dtype.utterances.txt trn.txt\n",
    "cp $dpath.test$dtype.utterances.txt tst.txt\n",
    "\n",
    "cp $dpath.train$dtype.conll.txt trn.conll\n",
    "cp $dpath.test$dtype.conll.txt tst.conll"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trn_data = read_corpus('trn.txt')\n",
    "trn_lex = cutoff(trn_data)\n",
    "\n",
    "with open('isyms.trn.txt', 'w') as f:\n",
    "    f.write(\"\\n\".join(trn_lex) + \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "ngramsymbols isyms.trn.txt isyms.txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "farcompilestrings \\\n",
    "    --symbols=isyms.txt \\\n",
    "    --keep_symbols \\\n",
    "    --unknown_symbol='<UNK>' \\\n",
    "    trn.txt trn.far\n",
    "\n",
    "farcompilestrings \\\n",
    "    --symbols=isyms.txt \\\n",
    "    --keep_symbols \\\n",
    "    --unknown_symbol='<UNK>' \\\n",
    "    tst.txt tst.far"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generating Output Symbol Table (`osyms.txt`)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "types = get_chunks('trn.conll')\n",
    "\n",
    "with open('osyms.u.lst.txt', 'w') as f:\n",
    "    f.write(\"O\" + \"\\n\")\n",
    "    for c in sorted(list(types)):\n",
    "        f.write(\"B-\"+ c + \"\\n\")\n",
    "        f.write(\"I-\"+ c + \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "ngramsymbols osyms.u.lst.txt osyms.txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create test set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "wdir='wdir'\n",
    "mkdir -p $wdir\n",
    "\n",
    "farextract --filename_prefix=\"$wdir/\" tst.far\n",
    "cp $wdir/tst.txt-0001 sent.fsa\n",
    "\n",
    "fstprint sent.fsa"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create training data for language model:\n",
    "- If using data set without stop words, set variable `data_type` equal to `_no_stop_word` to avoid applying `cutoff` function (avoid losing tags with low occurrences in data set)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_type = ''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trn = read_corpus_conll('trn.conll')\n",
    "tags = get_column(trn, column=-1)\n",
    "\n",
    "with open('trn.t.txt', 'w') as f:\n",
    "    for s in tags:\n",
    "        f.write(\" \".join(s) + \"\\n\")\n",
    "        \n",
    "tlex = cutoff(tags)\n",
    "with open('osyms.t.lst.txt', 'w') as f:\n",
    "    f.write(\"\\n\".join(tlex) + \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "# make symbol table\n",
    "ngramsymbols osyms.t.lst.txt osyms.t.txt\n",
    "# compile data into FAR again\n",
    "farcompilestrings \\\n",
    "    --symbols=osyms.t.txt \\\n",
    "    --keep_symbols \\\n",
    "    --unknown_symbol='<UNK>' \\\n",
    "    trn.t.txt trn.t.far"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Let's train a unigram language model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "ngramcount --order=3 trn.t.far trn.t1.cnt\n",
    "ngrammake trn.t1.cnt t1.lm\n",
    "ngramprint --symbols=osyms.t.txt --negativelogs t1.lm t1.probs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Let's create a new $\\lambda_{W2T}$ (let's call it $\\lambda_{W2T_{T}}$ for \"tags\"):\n",
    "    - following the same procedure we followed for $\\lambda_{W2T_{U}}$, but using:\n",
    "        - as input symbol table (`isyms.txt`)\n",
    "        - as output symbol table (`t.osyms.txt`)\n",
    "    - allowing `<unk> <unk>` and *word*-`<unk>` arcs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "make_w2t_mle('t1.probs', out='t1_mle.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "fstcompile --isymbols=osyms.t.txt --osymbols=osyms.t.txt --keep_isymbols --keep_osymbols t1_mle.txt t1_mle.bin"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "make_w2t('isyms.txt', 'osyms.t.txt', out='w2t_t.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Let's compile it\n",
    "fstcompile \\\n",
    "    --isymbols=isyms.txt \\\n",
    "    --osymbols=osyms.t.txt \\\n",
    "    --keep_isymbols \\\n",
    "    --keep_osymbols \\\n",
    "    w2t_t.txt w2t_t.bin\n",
    "\n",
    "fstinfo w2t_t.bin | head -n 8"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "fstcompose sent.fsa w2t_t.bin | fstcompose - t1.lm | fstshortestpath | fstrmepsilon | fsttopsort | fstprint"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "wdir='wdir'\n",
    "farr=($(ls $wdir))\n",
    "\n",
    "for f in ${farr[@]}\n",
    "do\n",
    "    fstcompose $wdir/$f w2t_t.bin | fstcompose - t1.lm |\\\n",
    "        fstshortestpath | fstrmepsilon | fsttopsort | fstprint --isymbols=isyms.txt\n",
    "done > w2t_t.t1.out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "refs = read_corpus_conll('tst.conll')\n",
    "hyps = read_fst4conll('w2t_t.t1.out')\n",
    "\n",
    "results = evaluate(refs, hyps)\n",
    "\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- The model still has $F_1=0$, since `O` is the tag with highest prior.\n",
    "- Observe the weights in the output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3.4. Exercises\n",
    "- Compare sizes of $\\lambda_{W2T_{U}}$ and $\\lambda_{W2T_{T}}$ for `# of arcs`\n",
    "- Unigram models & $\\lambda_{W2T}$\n",
    "    - Test pipeline: $\\lambda = \\lambda_{W} \\circ \\lambda_{W2T_{U}} \\circ \\lambda_{LM_{1}}$\n",
    "\n",
    "\n",
    "- Bigram models: train a *tag* bigram model (let's call it $\\lambda_{LM_{2}}$)\n",
    "\n",
    "    - Test pipeline with $\\lambda_{W2T_{U}}$: $\\lambda = \\lambda_{W} \\circ \\lambda_{W2T_{U}} \\circ \\lambda_{LM_{2}}$\n",
    "    - Test pipeline with $\\lambda_{W2T_{T}}$: $\\lambda = \\lambda_{W} \\circ \\lambda_{W2T_{T}} \\circ \\lambda_{LM_{2}}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3.5. Maximum Likelihood Estimation (Emission Probabilities)\n",
    "- So far we haven't explored the relation between input and output\n",
    "- The next thing we can do is to expose our model to observations and estimate $p(w_{i}|t_{i})$ from data.\n",
    "- We can use `ngramcount` and `ngrammake` to make a smoothed probability model (we are using default, i.e. no parameters). \n",
    "- We need to estimate probabilities like we would estimate bigram probabilities, thus:\n",
    "    - prepare lexicon with *tags* and *words*\n",
    "    - read CoNLL format corpus into far (token per line, preprocessed)\n",
    "    - count bigrams\n",
    "    - make a bigram language model\n",
    "    - print bigrams with weights (negative log probabilities)\n",
    "    - choose bigrams (it will contain unigrams, as well as `<s>` and `</s>` bigrams)\n",
    "    - convert to FST & compile\n",
    "    \n",
    "- Let's call the model $\\lambda_{W2T_{MLE}}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "# lets use our symbol tables (since they both have been applied cut-off)\n",
    "cat isyms.txt osyms.t.txt | cut -f 1 | sort | uniq > msyms.m.lst.txt\n",
    "ngramsymbols msyms.m.lst.txt msyms.t.txt\n",
    "\n",
    "# let's convert data to ngrams\n",
    "cat trn.conll | sed '/^$/d' | awk '{print $2,$1}' > trn.w2t.txt\n",
    "\n",
    "# compile to far\n",
    "farcompilestrings \\\n",
    "    --symbols=msyms.t.txt \\\n",
    "    --keep_symbols \\\n",
    "        --unknown_symbol='<UNK>' \\\n",
    "    trn.w2t.txt trn.w2t.far\n",
    "    \n",
    "# count bigrams\n",
    "ngramcount --order=2 trn.w2t.far trn.w2t.cnt\n",
    "# make a model\n",
    "ngrammake trn.w2t.cnt trn.w2t.lm\n",
    "\n",
    "# print ngram probabilities as negative logs\n",
    "ngramprint \\\n",
    "    --symbols=msyms.t.txt\\\n",
    "    --negativelogs \\\n",
    "    trn.w2t.lm trn.w2t.probs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Let's define a python function to convert probabilities printout to W2T FST"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "make_w2t_mle('trn.w2t.probs', out='trn.w2t_mle.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "fstcompile \\\n",
    "    --isymbols=osyms.t.txt \\\n",
    "    --osymbols=isyms.txt \\\n",
    "    --keep_isymbols \\\n",
    "    --keep_osymbols \\\n",
    "    trn.w2t_mle.txt w2t_mle.bin\n",
    "    \n",
    "# we need to invert it to have words on input\n",
    "fstinvert w2t_mle.bin w2t_mle.inv.bin\n",
    "\n",
    "fstinfo w2t_mle.inv.bin | head -n 8"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Testing\n",
    "Let's test it:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "# fstcompose sent.fsa w2t_mle.inv.bin | fstprint\n",
    "fstprint sent.fsa"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "fstcompose sent.fsa w2t_mle.inv.bin | fstprint"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "wdir='wdir'\n",
    "farr=($(ls $wdir))\n",
    "\n",
    "for f in ${farr[@]}\n",
    "do\n",
    "#  fstcompose $wdir/$f w2t_mle.inv.bin | fstshortestpath | fstrmepsilon | fsttopsort | fstprint\n",
    "  fstcompose $wdir/$f w2t_mle.inv.bin | fstcompose - t1.lm |  fstshortestpath | fstrmepsilon | fsttopsort | fstprint\n",
    "#  fstcompose $wdir/$f w2t_mle.inv.bin | fstcompose - t1_mle.bin | fstshortestpath | fstrmepsilon | fsttopsort | fstprint\n",
    "done > w2t_t.t1.mle_full.out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "refs = read_corpus_conll('tst.conll')\n",
    "hyps = read_fst4conll('w2t_t.t1.mle_full.out')\n",
    "\n",
    "results = evaluate(refs, hyps)\n",
    "\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- The pipeline above represents \n",
    "\n",
    "$$p(t_{1}^{n}|w_{1}^{n}) \\approx \\prod_{i=1}^{n}{p(w_i|t_i)}$$\n",
    "\n",
    "- To extend it to unigram tagging model we need to compose it with  $\\lambda_{LM_{1}} = p(t_i)$ \n",
    "\n",
    "$$\\lambda = \\lambda_{W} \\circ \\lambda_{W2T_{MLE}} \\circ \\lambda_{LM_{1}}$$\n",
    "\n",
    "$$p(t_{1}^{n}|w_{1}^{n}) \\approx \\prod_{i=1}^{n}{p(w_i|t_i)p(t_i)}$$ "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "fstcompose sent.fsa w2t_mle.inv.bin | fstcompose - t1.lm | fstshortestpath | fstrmepsilon | fsttopsort | fstprint"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 1: Maximum Likelihood Estimation\n",
    "- using `ngramprint` verify the Maximum Likelihood Estimation method (without `--negativelogs` it prints raw probabilities)\n",
    "    - print bigram counts from $\\lambda_{W2T_{MLE}}$ (output of `ngramcount`)\n",
    "    - print unigram counts for either from $\\lambda_{LM_{1}}$ or $\\lambda_{W2T_{MLE}}$ (output of `ngramcount`)\n",
    "    - using these counts compute probability of $p($ `brad|B-actor.name` $)$\n",
    "    - extract probability of $p($ `brad|B-actor.name` $)$ from $\\lambda_{W2T_{MLE}}$ (output of `ngrammake`)\n",
    "    - compare values\n",
    "    - repeat the procedure using counts from methods developed for the lab on ngram modeling."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise 2: Markov Model Tagger\n",
    "- Evaluate the MLE pipeline using bigram model on tags, i.e.\n",
    "\n",
    "$$\\lambda = \\lambda_{W} \\circ \\lambda_{W2T_{MLE}} \\circ \\lambda_{LM_{2}}$$\n",
    "\n",
    "$$p(t_{1}^{n}|w_{1}^{n}) \\approx \\prod_{i=1}^{n}{p(w_i|t_i)p(t_i|t_{i-1})}$$ \n",
    "\n",
    "- compare performances to the HMM tagger from previous lab (NLTK)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Joint Distribution Modeling\n",
    "\n",
    "As we have seen, sequence labeling for Language Understanding could be approached using Hidden Markov Models (similar to Part-of-Speech Tagging), and to models it as in the table below (__HMM__). Stochastic Conceptual Language Models for Spoken Language Understanding in [Raymond & Riccardi (2007)](https://disi.unitn.it/~riccardi/papers2/IS07-GenerDiscrSLU.pdf) (__R&R__) model it jointly.\n",
    "\n",
    "\n",
    "| Model   | Equation |\n",
    "|:--------|:----------\n",
    "| __HMM__ | $$p(t_{1}^{n}|w_{1}^{n}) \\approx \\prod_{i=1}^{n}{p(w_i|t_i)p(t_i|t_{i-N+1}^{i-1})}$$\n",
    "| __R&R__ | $$p(w_{1}^n,t_{1}^{n}) \\approx \\prod_{i=1}^{n}{p(w_{i}t_{i}|w_{i-N+1}^{i-1}t_{i-N+1}^{i-1})}$$\n",
    "\n",
    "\n",
    "From implementation perspective, joint modeling implies the following:\n",
    "- we need to train $\\lambda_{SCLM}$ on word-tag pairs\n",
    "    - create corpus in a format for estimating $p(w_i,t_i|w_{i-N+1}^{i-1}t_{i-N+1}^{i-1})$\n",
    "    - create symbol tables\n",
    "- we need to change $\\lambda_{W2T}$ to output *word-tag* pairs (let's call it $\\lambda_{W2WT}$)\n",
    "    - create FST like above for $\\lambda_{W2WT}$ ($\\lambda_{W2WT_{WT}}$ - to differentiate from $\\lambda_{W2WT_{U}}$ that contains all possible combinations)\n",
    "- we also need to change our input symbol table to accommodate OOV words due to joint modeling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Preparing Symbol Tables\n",
    "- Let's create output symbol table the same way we did for $\\lambda_{W2T_{T}}$\n",
    "- Let's create input symbol taking $w$ from the $w,t$ pair"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create training data in utterance-per-line format for output symbols (w+t)\n",
    "trn = read_corpus_conll('trn.conll')\n",
    "wt_sents = [[\"+\".join(w) for w in s] for s in trn]\n",
    "wt_osyms = cutoff(wt_sents)\n",
    "wt_isyms = [w.split('+')[0] for w in wt_osyms]\n",
    "\n",
    "with open('trn.wt.txt', 'w') as f:\n",
    "    for s in wt_sents:\n",
    "        f.write(\" \".join(s) + \"\\n\")\n",
    "        \n",
    "with open('osyms.wt.lst.txt', 'w') as f:\n",
    "    f.write(\"\\n\".join(wt_osyms) + \"\\n\")\n",
    "    \n",
    "with open('isyms.wt.lst.txt', 'w') as f:\n",
    "    f.write(\"\\n\".join(wt_isyms) + \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "ngramsymbols osyms.wt.lst.txt osyms.wt.txt\n",
    "ngramsymbols isyms.wt.lst.txt isyms.wt.txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Let's:\n",
    "    - compile our processed data into FAR\n",
    "    - train ngram language models on it - $\\lambda_{SCLM}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Training Conceptual Language Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "# compile data into FAR\n",
    "farcompilestrings \\\n",
    "    --symbols=osyms.wt.txt \\\n",
    "    --keep_symbols \\\n",
    "    --unknown_symbol='<UNK>' \\\n",
    "    trn.wt.txt trn.wt.far\n",
    "\n",
    "# train ngram model\n",
    "ngramcount --order=2 trn.wt.far trn.wt.cnt\n",
    "ngrammake trn.wt.cnt wt2.lm\n",
    "ngraminfo wt2.lm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Building W2WT FST"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Let's build unweighted $\\lambda_{W2WT_{WT}}$, using\n",
    "    - input symbol table `isyms.wt.txt`\n",
    "    - output symbol table `osyms.wt.txt`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_w2t_wt(isyms, sep='+', out='w2wt.tmp'):\n",
    "    special = {'<epsilon>', '<s>', '</s>'}\n",
    "    oov = '<unk>'  # unknown symbol\n",
    "    state = '0'    # wfst specification state\n",
    "    fs = \" \"       # wfst specification column separator\n",
    "    \n",
    "    ist = sorted(list(set([line.strip().split(\"\\t\")[0] for line in open(isyms, 'r')]) - special))\n",
    "    \n",
    "    with open(out, 'w') as f:\n",
    "        for e in ist:\n",
    "            f.write(fs.join([state, state, e.split('+')[0], e]) + \"\\n\")\n",
    "        f.write(state + \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "make_w2t_wt('osyms.wt.txt', out='w2wt_wt.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Let's compile it\n",
    "fstcompile \\\n",
    "    --isymbols=isyms.wt.txt \\\n",
    "    --osymbols=osyms.wt.txt \\\n",
    "    --keep_isymbols \\\n",
    "    --keep_osymbols \\\n",
    "    w2wt_wt.txt w2wt_wt.bin\n",
    "\n",
    "fstinfo w2wt_wt.bin | head -n 8"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Lets test the whole $\\lambda_{W} \\circ \\lambda_{W2WT_{WT}} \\circ \\lambda_{SCLM_{2}}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Preparing Test Data\n",
    "- since we have changed input symbol table we need to recompile & extract our test data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "farcompilestrings \\\n",
    "    --symbols=isyms.wt.txt \\\n",
    "    --keep_symbols \\\n",
    "    --unknown_symbol='<UNK>' \\\n",
    "    tst.txt tst.wt.far\n",
    "\n",
    "wdir='wdir_wt'\n",
    "mkdir -p $wdir\n",
    "\n",
    "farextract --filename_prefix=\"$wdir/\" tst.wt.far\n",
    "cp $wdir/tst.txt-0001 sent.wt.fsa\n",
    "\n",
    "fstprint sent.wt.fsa"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "fstcompose sent.wt.fsa w2wt_wt.bin | fstcompose - wt2.lm | fstshortestpath | fstrmepsilon | fsttopsort | fstprint"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Evaluation\n",
    "- Since on the output we have `word+tag`, we need to post-process the output for evaluation\n",
    "- the function `read_fst4conll` already has that functionality via `split=True` and `sep='+'`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "wdir='wdir_wt'\n",
    "farr=($(ls $wdir))\n",
    "\n",
    "for f in ${farr[@]}\n",
    "do\n",
    "    fstcompose $wdir/$f w2wt_wt.bin | fstcompose - wt2.lm |\\\n",
    "        fstshortestpath | fstrmepsilon | fsttopsort | fstprint --isymbols=isyms.wt.txt\n",
    "done > w2wt_wt.wt2.out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "refs = read_corpus_conll('tst.conll')\n",
    "hyps = read_fst4conll('w2wt_wt.wt2.out', split=True)\n",
    "\n",
    "results = evaluate(refs, hyps)\n",
    "\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise: Full $\\lambda_{W2WT_{U}}$\n",
    "- Implement $\\lambda_{W2WT_{U}}$ using 'full' input and output symbol tables (`isyms.txt` and `osyms.txt`)\n",
    "- Test the pipeline: $\\lambda_{W} \\circ \\lambda_{W2WT_{U}} \\circ \\lambda_{SCLM_{2}}$\n",
    "    - Observe the issues"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise\n",
    "- Compare each pipeline in terms of:\n",
    "    - size of input symbol table\n",
    "    - size of output symbol table\n",
    "    - size (number of arcs) of $\\lambda_{W2T}$\n",
    "    - size of $\\lambda_{*LM}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Common Improvements\n",
    "\n",
    "- Training an ngram language model on data that contains tags only (i.e. $\\lambda_{*LM}$) has one __big issue__: the out-of-span tag (`'O'`) is very frequent, consequently, there is not enough context to learn a good ngram model. \n",
    "- Joint modeling of words and tags, i.e. $\\lambda_{SCLM}$, on the other hand, has a very specific (and less frequent context).\n",
    "- There are two common enhancements to these models:\n",
    "    - removing out-of-span tag `'O'` from the $\\lambda_{*LM}$ to provide context for other tags\n",
    "    - generalization of input into __classes__, i.e. $\\lambda_{G}$, so that the data is less sparse"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Input Generalization (Normalization)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Th Language Understanding pipeline (as presented during the lectures) is \n",
    "\n",
    "$$\\lambda = \\lambda_{W} \\circ \\lambda_{G} \\circ \\lambda_{W2T} \\circ \\lambda_{*LM}$$\n",
    "\n",
    "The function of $\\lambda_{G}$ is this pipeline is to *generalize* the input, reducing sparsity."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Normalization\n",
    "In Natural Language Processing it is common to __normalize__ (pre-process) the input data to reduce sparsity (e.g. [textacy](https://chartbeat-labs.github.io/textacy/build/html/api_reference/text_processing.html))'s pre-processing). \n",
    "\n",
    "The __normalization__ replaced all members of the __infinite set__ with a single __unique token__ with respect to a __common pattern__. It is not possible to learn a good model for each possible number, for instance.\n",
    "\n",
    "- The example \"entities\" that have common pattern are:\n",
    "    - numbers\n",
    "    - emails\n",
    "    - url\n",
    "    - phone numbers\n",
    "    - credit card numbers\n",
    "    - etc.\n",
    "\n",
    "These \"entities\" are generally captured using __regular expressions__."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Lookup Tables\n",
    "Lookup tables provide a convenient way to generalize members of __large__ and __known set__ of entities. The common examples are *cities*, *countries*, *airport codes*, *movie names*, etc.\n",
    "Even though these sets are potentially infinite, the lists of cities and movie names are generally available as external __Knowledge Bases__, and it is possible to check membership of a token."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### [(Named) Entity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition)\n",
    "> Named-entity recognition (NER) (also known as entity identification, entity chunking and entity extraction) is a subtask of information extraction that seeks to locate and classify named entity mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.\n",
    "\n",
    "__NER__ also covers entities that are covered by *normalization*, as it is a matter of approach (regex vs. sequence labeling).\n",
    "There are several NLP tasks that fall under this category, specified with respect to the type of entity:\n",
    "    - TIMEX - temporal expressions\n",
    "    - ENAMEX - named entities \n",
    "    - NUMEX - numerical expressions\n",
    "    - etc. (e.g. protein names in BioMedical Domain)\n",
    "\n",
    "The task is similar to Concept Tagging, with an __important__ differences: \n",
    "- the same entity (from NER perspective) may belong to different classes in the target domain: \n",
    "    - e.g. in `NL2SparQL4NLU`: `actor.name`, `producer.name`, `director.name` are subclasses of a `PERSON`\n",
    "\n",
    "Consequently, the output of such systems could be used as input for Concept Tagging."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise: Lab\n",
    "- Implement number generalization to map all numerical expressions in input to `<num>`\n",
    "- Evaluate the pipeline with this step\n",
    "- Observe sizes of input and output symbol tables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}